import os, sys
import subprocess
import numpy as np
import pandas as pd
import scipy.io as sio
import dask.dataframe as dd
from stg.campaign import isNotBlank, new_folder
from joblib import Parallel, delayed, parallel_backend


def wrapper_scavetool(input_files_directory, output_directory, output_filename):
    """

    Try to convert scalar (*.sca) and vector (*.vec) statistics generated by OMNET++ tool scavetool.

    """
    # Go to OMNET++ results folder
    os.chdir(input_files_directory)
    output_cvs_file = os.path.join(output_directory, output_filename)

    # Find scavetool OMNET++ tool to generate .csv file from scalar and vector statistics
    command = 'whereis' if os.name != 'nt' else 'which'
    r = subprocess.getoutput('{0} {1}'.format(command, 'scavetool'))
    app_instance = (r.strip('scavetool:').strip()).split(' ')
    path = app_instance[1]  # TO DO in case more than one OMNET installation is found

    # Execute scavetool

    cmd = '{}scavetool x -v *.sca *.vec -o {}'.format(path, output_cvs_file)
    print(cmd)
    os.system(cmd)


def merge_files(input_files_directory, output_directory, output_filename, additional_files_path, max_processes):
    """

    Merge repetitions executed per scenario in one output file.
    Add statistics (mean, standard deviation).

    Input: Directory with results files. File with evaluation structure.csv (i.e., Parameters evaluated in project)
    Output: One output file (*.npy, *.mat *.csv).

    """

    # Read simulation campaign results in results folder
    result_files = os.listdir(input_files_directory)
    # List additional files directory
    variables_path = os.path.join(additional_files_path, 'variables.txt')
    structure_file_path = os.path.join(additional_files_path, 'structure.csv')

    omnet_files = False
    for file in result_files:
        if file.endswith(".sca") or file.endswith(".vec"):
            omnet_files = True
            break

    if omnet_files:

        # Call OMNET scavetool tool
        wrapper_scavetool(input_files_directory, output_directory, output_filename)
        print("Output file '{}' generated with scavetool from {} OMNeT++ result files\n".format(output_filename,
                                                                                                len(result_files)))

    else:

        # Read iteration parameter to insert in file structure.csv
        iteration_parameters_list = get_iteration_parameters(variables_path)

        # Read structure.csv
        e_parameters = get_parameters(structure_file_path)

        # results data frame
        total_results_df = pd.DataFrame()

        # No paralelized
        #results_df = results_df.append([build_results_df(input_files_directory, file, e_parameters, iteration_parameters_list) for file in result_files])

        with parallel_backend("loky"):
            total_results_df = total_results_df.append(Parallel(n_jobs=max_processes, verbose=1)(delayed(build_results_df)(input_files_directory, file, e_parameters, iteration_parameters_list) for file in result_files))
        save_simulation_results(total_results_df, output_directory, output_filename)
        print("Output file '{}' generated from {} result files\n".format(output_filename, len(result_files)))


def get_iteration_parameters(variables_path):
    """

    Read iteration parameters from file as part of the structure.csv

    """

    with open(variables_path, 'r') as ini_file:
        # iteration variables.txt
        iter_vararibles_list = []
        for line in ini_file:
            if isNotBlank(line):
                iter_vararible, value = line.split('=')
                iter_vararible = iter_vararible.strip().split('.')
                iter_vararibles_list.append(iter_vararible[-1])
    return iter_vararibles_list


"""

def build_results_df(input_files_directory, filename, structure, iteration_parameters_list):
    

    #Build dataframes from result files.
    #In case of numeric types, dataframes are constructed accordingly
    #Additional columns are add (filename, scenario, repetition) from the file name for grouping purposes.
    

    iteration_parameters_list = iteration_parameters_list[::-1]
    # read data and build a dataframe
    with open(os.path.join(input_files_directory, filename), 'r') as file:
        value = [line.strip().split(',') for line in file]
    values = np.array(value)

    if len(values[0]) == len(structure):
        # build dataframe from result file and structure.csv file (contains the columns names)
        df_results = pd.DataFrame(values, columns=structure)
        # assign numeric type to columns
        for column in df_results.columns:
            # try to convert to numeric if first value is numeric
            if type(parse_if_number(df_results[column].iloc[0])) == float:
                df_results[column] = pd.to_numeric(df_results[column], errors='coerce')

        # insert columns with filename, scenario and repetitions per execution
        name_components = filename.split(',')
        df_results.insert(0, 'name', filename)

        for i, component in enumerate(name_components):
            if i == 0:
                df_results.insert(i + 1, 'scenario', component)
            if i >= 1:
                if i + 1 == len(name_components):
                    df_results.insert(i + 1, 'repetition', component)
                else:
                    df_results.insert(i + 1, '{}'.format(iteration_parameters_list[i-1]), component)
        return df_results

    else:
        print('Number of structure.csv must be equal to structure.csv in results files!!!')
        sys.exit()
"""


def build_results_df(input_files_directory, filename, structure, iteration_parameters_list):

    #Build dataframes from result files.
    #In case of numeric types, dataframes are constructed accordingly
    #Additional columns are add (filename, scenario, repetition) from the file name for grouping purposes.

    iteration_parameters_list = iteration_parameters_list[::-1]
    # read data and build a dataframe
    with open(os.path.join(input_files_directory, filename), 'r') as file:
        value = [line.strip().split(',') for line in file]
    values = np.array(value)

    if len(values[0]) == len(structure):
        # build dataframe from result file and structure.csv file (contains the columns names)
        df_results = pd.DataFrame(values, columns=structure)
        # assign numeric type to columns
        for column in df_results.columns:
            # try to convert to numeric if first value is numeric
            if type(parse_if_number(df_results[column].iloc[0])) == float:
                df_results[column] = pd.to_numeric(df_results[column], errors='coerce')

        # insert columns with filename, scenario and repetitions per execution
        name_components = filename.split(',')

        df_results.insert(0, 'name', filename)

        for i, component in enumerate(name_components):
            component = component.strip('-')
            component = component.strip('.csv')

            if i == 0:
                df_results.insert(i + 1, 'scenario', component)
            if i >= 1:
                if i + 1 == len(name_components):
                    df_results.insert(i + 1, 'repetition', component)
                else:
                    df_results.insert(i + 1, '{}'.format(iteration_parameters_list[i-1]), np.int(component))
        # Add column with distance
        tx_df = df_results[df_results.TR == 'tx']
        tx_df = tx_df[tx_df.NodeID == 0]
        x = tx_df[tx_df.MsgTime <= 214].x
        x = x.astype('float64')
        y = tx_df[tx_df.MsgTime <= 214].y
        y = y.astype('float64')
        # max distance of rx message
        df_results = df_results[df_results.TR == 'rx']
        df_results['Distance'] = ((x - df_results.x)**2 + (y - df_results.y)**2)**(1/2)

        if not df_results.empty:
            maximumdist = max(df_results['Distance'])
            #rx_df['MaxDistance'] = maximumdist
        else:
            maximumdist = 0
        df_results.insert(1, 'MaxDistance', maximumdist)

        # number of nodes
        df_results.drop_duplicates(subset='NodeID', keep="last", inplace=True)
        num_nodes = int(df_results.NodeID.count())
        #rx_df['NumNodes'] = num_nodes
        df_results.insert(2, 'NumberNodes', num_nodes)
        return df_results


def parse_if_number(s):
    """

    Return a float, True/False or None

    """
    try:
        return float(s)
    except:
        return True if s == 'true' else False if s == 'false' else s if s else None


def get_parameters(eval_path):
    """

    Return a list of structure.csv to sort the results table.
    It must be consistent with gathered values in the project.

    Structure in structure.csv external file:

            parm1,parm2,parm3,.....

    """

    # Read structure.csv
    with open(eval_path, 'r') as eval_file:
        for line in eval_file:
            if isNotBlank(line):
                eval_param = line.strip('\n').split(',')
    return eval_param


def save_simulation_results(results_summary, output_directory, file_name):
    """

    Export results to file to supported file extension.
    The extension must be included in the filename.
    By default .npy is selected.

    Supported extensions:
         .npy (Numpy file), .mat (Matlab file) or csv (Comma-separated values) by default

    """

    new_folder(output_directory)
    name, ext = file_name.split('.')
    if ext in ['mat', 'MAT']:
        sio.savemat(os.path.join(output_directory, file_name), results_summary)
    else:
        sim_summary = dd.from_pandas(results_summary, npartitions=20)
        if ext in ['npy', 'NPY']:
            np.save(os.path.join(output_directory, file_name), sim_summary)
        else:
            # default csv file
            sim_summary.to_csv(os.path.join(output_directory, file_name), index=None, header=True, single_file=True)
